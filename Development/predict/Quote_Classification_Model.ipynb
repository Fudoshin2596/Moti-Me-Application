{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Quote_Classification_Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9bVxnVNTjpV"
      },
      "source": [
        "# !pip install --upgrade pip"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vx5FXv9vsBm"
      },
      "source": [
        "# pip install -q transformers==3"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvMlDjs63TBG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88406901-6f4e-4b22-bd5e-2617ccb20705"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import json, re\n",
        "from tqdm.notebook import tqdm\n",
        "from uuid import uuid4\n",
        "import tarfile\n",
        "import io\n",
        "import os\n",
        "import sys\n",
        "import datetime\n",
        "import argparse\n",
        "\n",
        "## Torch Modules\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchtext.data as data\n",
        "import torchtext.datasets as datasets\n",
        "import torch.autograd as autograd\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.data import Field, TabularDataset, BucketIterator, Iterator #Provides several easy to use Swiss army knife iterators.\n",
        "from torchtext.vocab import GloVe\n",
        "\n",
        "## PyTorch Transformer\n",
        "from transformers import MobileBertTokenizer, MobileBertModel, AdamW, get_linear_schedule_with_warmup, MobileBertForSequenceClassification, MobileBertConfig\n",
        "\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import logging\n",
        "logging.getLogger(\"transformers.tokenization_utils_base\").setLevel(logging.ERROR)\n",
        "\n",
        "## Mount Drive into Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMZEOUL7qGQf"
      },
      "source": [
        "data_path = \"/content/drive/MyDrive\"\n",
        "output_path = \"/content/drive/MyDrive/outputs\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkRqK2sM3evs"
      },
      "source": [
        "## Importing Datasets"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uudmDs_U39YL"
      },
      "source": [
        "dataset_path = \"/content/drive/MyDrive/BalancedQuotesClean.csv\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MplSbbEhU4UI"
      },
      "source": [
        "dataset = pd.read_csv(dataset_path)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKfWQ3G1VJXk"
      },
      "source": [
        "# dataset.head()"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvNXPtT-4Xed"
      },
      "source": [
        "label_to_ix = {}\n",
        "for label in dataset.label:\n",
        "    for word in label.split():\n",
        "        if word not in label_to_ix:\n",
        "            label_to_ix[word]=len(label_to_ix)\n",
        "# label_to_ix"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xn6bHXlBxbIw"
      },
      "source": [
        "labels = list(label_to_ix.keys())\n",
        "num_labels = len(labels)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "ILyJ7zitrxd8",
        "outputId": "352bbc1a-494b-4bc5-ca0f-a1702cdb388f"
      },
      "source": [
        "df = dataset.copy()\n",
        "# Save preprocessed data, cropped to max length of the model.\n",
        "df['quote'] = df['quote'].apply(lambda x: \" \".join(x.split()[:512]))\n",
        "df['text'] = df['quote']\n",
        "del df['quote']\n",
        "# df['label'] = df['label'].map(label_to_ix)\n",
        "df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>aspirations</td>\n",
              "      <td>To sin offers repentance and forgiveness not t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>aspirations</td>\n",
              "      <td>Be calm in arguing for fierceness makes error ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>aspirations</td>\n",
              "      <td>For all of its uncertainty we cannot flee the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>aspirations</td>\n",
              "      <td>The way that a handful of corporations in Los ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>aspirations</td>\n",
              "      <td>Dreams have only one owner at a time. That's w...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         label                                               text\n",
              "0  aspirations  To sin offers repentance and forgiveness not t...\n",
              "1  aspirations  Be calm in arguing for fierceness makes error ...\n",
              "2  aspirations  For all of its uncertainty we cannot flee the ...\n",
              "3  aspirations  The way that a handful of corporations in Los ...\n",
              "4  aspirations  Dreams have only one owner at a time. That's w..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iN88xmUIsYZb"
      },
      "source": [
        "new_path=f\"{data_path}/train.csv\"\n",
        "df.to_csv(new_path)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOo0EJJelA1Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "805608d6-6ee3-4c14-a4a9-9d24927f2942"
      },
      "source": [
        "gc.collect()"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "126252"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGoQoAqqAnhp"
      },
      "source": [
        "## Check if Cuda is Available\n",
        "# Set random seed and set device to GPU.\n",
        "#choose the same seed to assure that our model will be roproducible\n",
        "seed_value = 876\n",
        "\n",
        "def seed_all(seed_value):\n",
        "    random.seed(seed_value) # Python\n",
        "    np.random.seed(seed_value) # cpu vars\n",
        "    torch.manual_seed(seed_value) # cpu  vars\n",
        "    \n",
        "    if torch.cuda.is_available(): \n",
        "        device = torch.device('cuda:0')\n",
        "        torch.cuda.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value) # gpu vars\n",
        "        torch.backends.cudnn.deterministic = True  #needed\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "    else:\n",
        "      device = torch.device('cpu')\n",
        "    return device"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KR-GhCDtZBJE"
      },
      "source": [
        "def parse_arguments():\n",
        "    p = argparse.ArgumentParser(description='Hyperparams for Classifier Training')\n",
        "    # learning\n",
        "    p.add_argument('-lr', type=float, default=0.001,\n",
        "                   help='initial learning rate')\n",
        "    p.add_argument('-epochs', type=int, default=5,\n",
        "                   help='number of epochs for train')\n",
        "    p.add_argument('-batch-size', type=int, default=32,\n",
        "                   help='batch size for training')\n",
        "    p.add_argument('-log-interval',  type=int, default=25,\n",
        "                   help='how many steps to wait before logging')\n",
        "    p.add_argument('-test-interval', type=int, default=200,\n",
        "                   help='how many steps to wait before testing')\n",
        "    p.add_argument('-save-interval', type=int, default=500,\n",
        "                   help='how many steps to wait before saving')\n",
        "    p.add_argument('-save-dir', type=str, default='snapshot',\n",
        "                   help='where to save the snapshot')\n",
        "    # model\n",
        "    p.add_argument('-model', type=str, default='ConvText',\n",
        "                   help='model name')\n",
        "    p.add_argument('-embed-dim', type=int, default=128,\n",
        "                   help='word embedding dimensions')\n",
        "    p.add_argument('-dropout', type=float, default=0.5,\n",
        "                   help='the probability for dropout')\n",
        "    # model - LSTM\n",
        "    p.add_argument('-hidden-dim', type=int, default=128,\n",
        "                   help='hidden state size')\n",
        "    p.add_argument('-n-layers', type=int, default=3,\n",
        "                   help='LSTM layer num')\n",
        "    p.add_argument('-attention-dim', type=int, default=10,\n",
        "                   help='attention dimensions')\n",
        "    # model - CNN\n",
        "    p.add_argument('-max-norm', type=float, default=3.0,\n",
        "                   help='l2 constraint of parameters [default: 3.0]')\n",
        "    p.add_argument('-n-kernel', type=int, default=100,\n",
        "                   help='number of each kind of kernel')\n",
        "    p.add_argument('-kernel-sizes', type=str, default='1',\n",
        "                   help='comma-separated kernel size to use for convolution')\n",
        "    p.add_argument('-static', action='store_true', default=False,\n",
        "                   help='fix the embedding')\n",
        "    # device\n",
        "    p.add_argument('-device', type=int, default=-1,\n",
        "                   help='device to use for iterate data, -1 mean cpu')\n",
        "    # option\n",
        "    p.add_argument('-snapshot', type=str, default='/content/snapshot/2020-12-21_02-52-26/snapshot_steps13000.pt',\n",
        "                   help='filename of model snapshot [default: None]')\n",
        "    p.add_argument('-predict', type=str, default=\"I saw the best minds of my generation destroyed by madness, starving hysterical naked.\",\n",
        "                   help='predict the sentence given')\n",
        "    p.add_argument('-test', action='store_true', default=True,\n",
        "                   help='train or test')\n",
        "    return p.parse_args(\"\")"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IV6wL7CJZBGk"
      },
      "source": [
        "# load SST dataset\n",
        "def sst(TEXT, LABEL, batch_size):\n",
        "    train, val, test = datasets.SST.splits(TEXT, LABEL, fine_grained=True)\n",
        "    TEXT.build_vocab(train, val, test)\n",
        "    LABEL.build_vocab(train, val, test)\n",
        "    train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
        "                                        (train, val, test),\n",
        "                                        batch_sizes=(batch_size,\n",
        "                                                     len(val),\n",
        "                                                     len(test)))\n",
        "    return train_iter, val_iter, test_iter\n",
        "\n",
        "\n",
        "# load imdb dataset\n",
        "def imdb(TEXT, LABEL, batch_size):\n",
        "    train, test = datasets.IMDB.splits(TEXT, LABEL)\n",
        "    TEXT.build_vocab(train, vectors=GloVe(name='6B', dim=300))\n",
        "    LABEL.build_vocab(train)\n",
        "    train_iter, test_iter = data.BucketIterator.splits(\n",
        "            (train, test), batch_size=batch_size,\n",
        "            shuffle=True, repeat=False)\n",
        "    print('len(train):', len(train))\n",
        "    print('len(test):', len(test))\n",
        "    return train_iter, test_iter\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# load custom dataset\n",
        "def customdata(TEXT, LABEL, batch_size):\n",
        "    device = seed_all(456)\n",
        "    # MODEL_NAME = 'google/mobilebert-uncased'\n",
        "    # tokenizer = MobileBertTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    # # Set tokenizer hyperparameters.\n",
        "    # MAX_SEQ_LEN = 128\n",
        "    # BATCH_SIZE = batch_size\n",
        "    # PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
        "    # UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n",
        "\n",
        "\n",
        "    # Define columns to read.\n",
        "    # LABEL = Field(sequential=False, use_vocab=False, batch_first=True)\n",
        "    # TEXT = Field(use_vocab=False, \n",
        "    #                   tokenize=tokenizer.encode, \n",
        "    #                   include_lengths=False, \n",
        "    #                   batch_first=True,\n",
        "    #                   fix_length=MAX_SEQ_LEN, \n",
        "    #                   pad_token=PAD_INDEX, \n",
        "    #                   unk_token=UNK_INDEX)\n",
        "\n",
        "    fields = {'text' : ('text', TEXT), 'label' : ('label', LABEL)}\n",
        "\n",
        "    # Read preprocessed CSV into TabularDataset and split it into train, test and valid.\n",
        "    train_data, val_data, test_data = TabularDataset(path=new_path, \n",
        "                                                      format='CSV', \n",
        "                                                      fields=fields, \n",
        "                                                      skip_header=False).split(split_ratio=[0.50, 0.25, 0.25], \n",
        "                                                                                stratified=True, \n",
        "                                                                                strata_field='label')\n",
        "\n",
        "    TEXT.build_vocab(train_data, vectors=GloVe(name='6B', dim=300))\n",
        "    LABEL.build_vocab(train_data)\n",
        "\n",
        "    # Create train and validation iterators.\n",
        "    train_iter, val_iter = BucketIterator.splits((train_data, val_data),\n",
        "                                                  batch_size=batch_size,\n",
        "                                                  device=device,\n",
        "                                                  shuffle=True,\n",
        "                                                  sort_key=lambda x: len(x.text), \n",
        "                                                  sort=True, \n",
        "                                                  sort_within_batch=False)\n",
        "\n",
        "    # Test iterator, no shuffling or sorting required.\n",
        "    test_iter = Iterator(test_data, batch_size=batch_size, device=device, train=False, shuffle=False, sort=False)\n",
        "\n",
        "    print('len(train):', len(train_data))\n",
        "    print('len(test):', len(test_data))\n",
        "    return train_iter, val_iter, test_iter"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCJfICIVZBEJ"
      },
      "source": [
        "class AttentionLSTM(nn.Module):\n",
        "    def __init__(self, args, n_vocab, embed_dim, n_classes, dropout=0.2):\n",
        "        super(AttentionLSTM, self).__init__()\n",
        "        print(\"Building Attention LSTM model...\")\n",
        "        self.n_layers = args.n_layers\n",
        "        self.hidden_dim = args.hidden_dim\n",
        "        self.attention_dim = args.attention_dim\n",
        "        self.v = nn.Parameter(torch.Tensor(self.attention_dim, 1))\n",
        "        self.m1 = nn.Linear(self.hidden_dim, self.attention_dim)\n",
        "        self.m2 = nn.Linear(self.hidden_dim, self.attention_dim)\n",
        "\n",
        "        self.embed = nn.Embedding(n_vocab, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, self.hidden_dim,\n",
        "                            num_layers=self.n_layers,\n",
        "                            dropout=dropout,\n",
        "                            batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.n = nn.Linear(self.hidden_dim + self.hidden_dim, self.hidden_dim)\n",
        "        self.output = nn.Linear(self.hidden_dim, n_classes)\n",
        "\n",
        "    def attention(self, h, h_t, i_size, b_size):\n",
        "        attention = []\n",
        "        for i in range(i_size):\n",
        "            m1 = self.m1(h[:,i,:])  # [b, e] -> [b, a]\n",
        "            m2 = self.m2(h_t)   # [b, h] -> [b, a]\n",
        "            a = torch.mm(F.tanh(m1 + m2), self.v)\n",
        "            attention.append(a)\n",
        "        attention = F.softmax(torch.stack(attention, 0))  # [i, b, 1]\n",
        "        context = torch.bmm(h.transpose(1, 2), attention.transpose(0,1))\n",
        "        return context.squeeze()\n",
        "\n",
        "    def forward(self, x):\n",
        "        b_size = x.size()[0]\n",
        "        i_size = x.size()[1]\n",
        "        state = self._init_state(b_size)\n",
        "        x = self.embed(x)  # [b, i, e]\n",
        "        out, h_t = self.lstm(x, state)  # out: [b, i, h]\n",
        "        c = self.attention(out, out[:, -1, :], i_size, b_size)\n",
        "        n = F.tanh(self.n(torch.cat([c, out[:, -1, :]], 1)))\n",
        "        self.dropout(n)\n",
        "        logit = self.output(n)\n",
        "        return logit\n",
        "\n",
        "    def _init_state(self, b_size=1):\n",
        "        weight = next(self.parameters()).data\n",
        "        return (\n",
        "            Variable(weight.new(self.n_layers, b_size, self.hidden_dim).zero_()),\n",
        "            Variable(weight.new(self.n_layers, b_size, self.hidden_dim).zero_())\n",
        "        )"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QO4HG6aWZiU7"
      },
      "source": [
        "class BasicLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "        Basic LSTM\n",
        "    \"\"\"\n",
        "    def __init__(self, args, n_vocab, embed_dim, n_classes, dropout=0.5):\n",
        "        super(BasicLSTM, self).__init__()\n",
        "        print(\"Building Basic LSTM model...\")\n",
        "        self.n_layers = args.n_layers\n",
        "        self.hidden_dim = args.hidden_dim\n",
        "\n",
        "        self.embed = nn.Embedding(n_vocab, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.lstm = nn.LSTM(embed_dim, self.hidden_dim,\n",
        "                            num_layers=self.n_layers, batch_first=True)\n",
        "        self.out = nn.Linear(self.hidden_dim, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b_size = x.size()[0]\n",
        "        h_0 = self._init_state(b_size=b_size)\n",
        "        x = self.embed(x)  #  [b, i] -> [b, i, e]\n",
        "        x, _ = self.lstm(x, h_0)  # [i, b, h]\n",
        "        h_t = x[:,-1,:]\n",
        "        self.dropout(h_t)\n",
        "        logit = self.out(h_t)  # [b, h] -> [b, o]\n",
        "        return logit\n",
        "\n",
        "    def _init_state(self, b_size=1):\n",
        "        weight = next(self.parameters()).data\n",
        "        return (\n",
        "            Variable(weight.new(self.n_layers, b_size, self.hidden_dim).zero_()),\n",
        "            Variable(weight.new(self.n_layers, b_size, self.hidden_dim).zero_())\n",
        "        )"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auEnVr55ZiSm"
      },
      "source": [
        "class  ConvText(nn.Module):\n",
        "    \"\"\"\n",
        "        Convolutional Neural Networks for Sentence Classification\n",
        "        https://arxiv.org/abs/1408.5882\n",
        "    \"\"\"\n",
        "    def __init__(self, args, n_vocab, embed_dim, n_classes, dropout=0.5):\n",
        "        super(ConvText,self).__init__()\n",
        "        print(\"Building Conv model...\")\n",
        "        self.args = args\n",
        "        c_out = args.n_kernel\n",
        "        kernels = args.kernel_sizes\n",
        "\n",
        "        self.embed = nn.Embedding(n_vocab, embed_dim)\n",
        "        self.convs = nn.ModuleList([nn.Conv2d(1, c_out, (k, embed_dim))\n",
        "                                   for k in kernels])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(len(kernels) * c_out, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)   #  [b, i] -> [b, i, e]\n",
        "        if self.args.static:\n",
        "            x = Variable(x)\n",
        "        x = x.unsqueeze(1)  #  [b, c_in, i, e]\n",
        "        #  [(b, c_out, i), ...] * len(kernels)\n",
        "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
        "        #  [(b, c_out), ...] * len(kernels)\n",
        "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n",
        "        x = torch.cat(x, 1)\n",
        "        x = self.dropout(x)  # (b, len(kernels) * c_out)\n",
        "        logit = self.fc(x)   # (b, o)\n",
        "        return logit"
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSv0Wj_YZiMn"
      },
      "source": [
        "# from classifiers import BasicLSTM, AttentionLSTM, ConvText\n",
        "# from args import parse_arguments\n",
        "# from utils import imdb\n",
        "\n",
        "classifiers = {\n",
        "    \"BasicLSTM\": BasicLSTM,\n",
        "    \"AttentionLSTM\": AttentionLSTM,\n",
        "    \"ConvText\": ConvText\n",
        "}\n",
        "\n",
        "def train(model, train_iter, val_iter, args):\n",
        "    \"\"\"train model\"\"\"\n",
        "    if args.cuda:\n",
        "        model.cuda()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "    steps = 0\n",
        "    model.train()\n",
        "    for epoch in range(1, args.epochs+1):\n",
        "        print(\"\\n\\nEpoch: \", epoch)\n",
        "        for batch in train_iter:\n",
        "            x, y = batch.text, batch.label\n",
        "            y.data.sub_(1)  # index align\n",
        "            if args.cuda:\n",
        "                x, y = x.cuda(), y.cuda()\n",
        "            optimizer.zero_grad()\n",
        "            logit = model(x)\n",
        "            loss = F.cross_entropy(logit, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            steps += 1\n",
        "            if steps % args.log_interval == 0:\n",
        "                corrects = (torch.max(logit, 1)\n",
        "                            [1].view(y.size()).data == y.data).sum()\n",
        "                accuracy = 100.0 * corrects/batch.batch_size\n",
        "                sys.stdout.write(\n",
        "                    '\\rBatch[{}] - loss: {:.6f}  acc: {:.4f}%({}/{})'.format(\n",
        "                    steps, loss.data.item(), accuracy, corrects, batch.batch_size))\n",
        "            if steps % args.test_interval == 0:\n",
        "                evaluate(model, val_iter, args)\n",
        "            if steps % args.save_interval == 0:\n",
        "                if not os.path.isdir(args.save_dir):\n",
        "                    os.makedirs(args.save_dir)\n",
        "                save_prefix = os.path.join(args.save_dir, 'snapshot')\n",
        "                save_path = '{}_steps{}.pt'.format(save_prefix, steps)\n",
        "                torch.save(model, save_path)\n",
        "\n",
        "def evaluate(model, val_iter, args):\n",
        "    \"\"\"evaluate model\"\"\"\n",
        "    model.eval()\n",
        "    corrects, avg_loss = 0, 0\n",
        "    for batch in val_iter:\n",
        "        x, y = batch.text, batch.label\n",
        "        y.data.sub_(1)  # index align\n",
        "        if args.cuda:\n",
        "            x, y = x.cuda(), y.cuda()\n",
        "        logit = model(x)\n",
        "        loss = F.cross_entropy(logit, y, size_average=False)\n",
        "        avg_loss += loss.data.item()\n",
        "        corrects += (torch.max(logit, 1)\n",
        "                     [1].view(y.size()).data == y.data).sum()\n",
        "    size = len(val_iter.dataset)\n",
        "    avg_loss = avg_loss / size\n",
        "    accuracy = 100.0 * corrects / size\n",
        "    print('\\nEvaluation - loss: {:.6f}  acc: {:.4f}%({}/{}) \\n'.format(\n",
        "          avg_loss, accuracy, corrects, size))\n",
        "    model.train() # return to training mode\n",
        "\n",
        "def predict(model, text, TEXT, LABEL):\n",
        "    device = seed_all(456)\n",
        "    MODEL_NAME = 'google/mobilebert-uncased'\n",
        "    tokenizer = MobileBertTokenizer.from_pretrained(MODEL_NAME)\n",
        "    \n",
        "    # from torch import Tensor\n",
        "    \"\"\"predict\"\"\"\n",
        "    assert isinstance(text, str)\n",
        "    model.eval()\n",
        "    \n",
        "    \n",
        "    encoded_review = tokenizer.encode_plus(\n",
        "    text,\n",
        "    max_length=128,\n",
        "    add_special_tokens=True,\n",
        "    return_token_type_ids=False,\n",
        "    pad_to_max_length=False,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='pt')\n",
        "    x = encoded_review['input_ids'].to(device)\n",
        "\n",
        "\n",
        "    # text = TEXT.tokenize(text)\n",
        "    # text = TEXT.preprocess(text)\n",
        "    # text = [[TEXT.vocab.stoi[x] for x in text]]\n",
        "    # x = torch.LongTensor(text).to(device)\n",
        "    # x.type(torch.DoubleTensor)\n",
        "    # x = TEXT.tensor_type(text)\n",
        "    # x = autograd.Variable(x, volatile=True)\n",
        "    # print(x)\n",
        "    output = model(x)\n",
        "    _, predicted = torch.max(output, 1)\n",
        "    return LABEL.vocab.itos[predicted.data.item()]\n",
        "\n",
        "def main1():\n",
        "    # get hyper parameters\n",
        "    args = parse_arguments()\n",
        "\n",
        "    # load data\n",
        "    print(\"\\nLoading data...\")\n",
        "    TEXT = data.Field(lower=True, batch_first=True)\n",
        "    LABEL = data.Field(sequential=False)\n",
        "    train_iter, val_iter, test_iter = customdata(TEXT, LABEL, args.batch_size)\n",
        "\n",
        "    # update args\n",
        "    args.n_vocab = n_vocab = len(TEXT.vocab)\n",
        "    args.n_classes = n_classes = len(LABEL.vocab) - 1\n",
        "    args.cuda = torch.cuda.is_available()\n",
        "    args.kernel_sizes = [int(k) for k in args.kernel_sizes.split(',')]\n",
        "    args.save_dir = os.path.join(args.save_dir,\n",
        "            datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
        "\n",
        "    # print args\n",
        "    print(\"\\nParameters:\")\n",
        "    for attr, value in sorted(args.__dict__.items()):\n",
        "        print(\"\\t{}={}\".format(attr.upper(), value))\n",
        "    \n",
        "    return args, train_iter, val_iter, test_iter, n_vocab, n_classes, TEXT, LABEL\n",
        "\n",
        "def main2(args, train_iter, val_iter, test_iter, n_vocab, n_classes, TEXT, LABEL):\n",
        "    # initialize/load the model\n",
        "    if args.snapshot is None:\n",
        "        classifier = classifiers[args.model]\n",
        "        classifier = classifier(args, n_vocab, args.embed_dim, n_classes, args.dropout)\n",
        "    else :\n",
        "        print('\\nLoading model from [%s]...' % args.snapshot)\n",
        "        try:\n",
        "            classifier = torch.load(args.snapshot)\n",
        "        except :\n",
        "            print(\"Sorry, This snapshot doesn't exist.\"); exit()\n",
        "    if args.cuda:\n",
        "        classifier = classifier.cuda()\n",
        "\n",
        "    # train, test, or predict\n",
        "    if args.predict is not None:\n",
        "        label = predict(classifier, args.predict, TEXT, LABEL)\n",
        "        print('\\n[Text]  {}[Label] {}\\n'.format(args.predict, label))\n",
        "    elif args.test :\n",
        "        try:\n",
        "            evaluate(classifier, test_iter, args)\n",
        "        except Exception as e:\n",
        "            print(\"\\nSorry. The test dataset doesn't  exist.\\n\")\n",
        "            print(e)\n",
        "    else :\n",
        "        print()\n",
        "        train(classifier, train_iter, val_iter, args)"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvrXxY_cZiJM",
        "outputId": "ab3b0aba-dc71-4470-fde8-0c562a413336"
      },
      "source": [
        "args, train_iter, val_iter, test_iter, n_vocab, n_classes, TEXT, LABEL = main1()"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loading data...\n",
            "len(train): 21015\n",
            "len(test): 10510\n",
            "\n",
            "Parameters:\n",
            "\tATTENTION_DIM=10\n",
            "\tBATCH_SIZE=32\n",
            "\tCUDA=True\n",
            "\tDEVICE=-1\n",
            "\tDROPOUT=0.5\n",
            "\tEMBED_DIM=128\n",
            "\tEPOCHS=5\n",
            "\tHIDDEN_DIM=128\n",
            "\tKERNEL_SIZES=[1]\n",
            "\tLOG_INTERVAL=25\n",
            "\tLR=0.001\n",
            "\tMAX_NORM=3.0\n",
            "\tMODEL=ConvText\n",
            "\tN_CLASSES=5\n",
            "\tN_KERNEL=100\n",
            "\tN_LAYERS=3\n",
            "\tN_VOCAB=24294\n",
            "\tPREDICT=I saw the best minds of my generation destroyed by madness, starving hysterical naked.\n",
            "\tSAVE_DIR=snapshot/2020-12-21_03-22-00\n",
            "\tSAVE_INTERVAL=500\n",
            "\tSNAPSHOT=/content/snapshot/2020-12-21_02-52-26/snapshot_steps13000.pt\n",
            "\tSTATIC=False\n",
            "\tTEST=True\n",
            "\tTEST_INTERVAL=200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97hE4dl9ZBB8",
        "outputId": "d15f0ee2-1560-4884-9584-172f9ec10726"
      },
      "source": [
        "main2(args, train_iter, val_iter, test_iter, n_vocab, n_classes, TEXT, LABEL)"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loading model from [/content/snapshot/2020-12-21_02-52-26/snapshot_steps13000.pt]...\n",
            "\n",
            "[Text]  I saw the best minds of my generation destroyed by madness, starving hysterical naked.[Label] aspirations\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mMMFJ5DLEB6"
      },
      "source": [
        "Epoch:  20\n",
        "Batch[12600] - loss: 0.098719  acc: 100.0000%(32/32)\n",
        "Evaluation - loss: 0.393841  acc: 86.9396%(9133/10505) \n",
        "\n",
        "Batch[12800] - loss: 0.400348  acc: 84.3750%(27/32)\n",
        "Evaluation - loss: 0.401343  acc: 86.6159%(9099/10505) \n",
        "\n",
        "Batch[13000] - loss: 0.271296  acc: 90.6250%(29/32)\n",
        "Evaluation - loss: 0.407512  acc: 86.7301%(9111/10505) \n",
        "\n",
        "Batch[13125] - loss: 0.096005  acc: 100.0000%(32/32)"
      ]
    }
  ]
}